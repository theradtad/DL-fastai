{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkO8MzdbM6LE4Q61gcjSeB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theradtad/DL-fastai/blob/main/Fastai_dl_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 1**\n",
        "\n",
        "Some notes:\n",
        "\n",
        "\n",
        "1.   Stochastic Gradient descent and Universal Approximation theorem.\n",
        "2.   Model has an architecture. Each has weights -> parameters which influence the output of a model.\n",
        "3. Epoch: One pass over the entire training split of the dataset.\n",
        "4. Batch: The number of inputs after which the models parameters are adjusted.\n",
        "5. Train: All the layers of the network are modified and at the same learning rate.\n",
        "6. Fine-tune: In the first epoch, only the newly added layers for the sake of our classification task are modified. In the next epochs, all parameters are modified but the learning rate of the newly added layers is more than the learning rate of the layers of the pretrained model. This is to ensure minimal changes to the existing model.\n",
        "7. Overfitting - when the model learns the training data. Upto a certain number of epochs, the error rate will decrease post which it starts increasing. Here, we are overfitting.\n",
        "8. Goal of training is for the model to learn the characterstic of the items in the training set but not the item itself.\n",
        "9. Training data vs validation data vs test-set: The training data is where the models parameters are adjusted. The validation data is on which the model is tested post training. At this point, we may choose to modify any of the hyperparameters and retrain the model. (learning rate, architecture, data augmentation - where existing data is slightly modified to increase the size of the training set.). At the very end, we run the model on the test set.\n"
      ],
      "metadata": {
        "id": "kMexpP-9bxZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 2**\n",
        "\n",
        "1. DriveTrain Approach when choosing a model: What is the objective? What is the action we can take to get to our object? What is the data we have or can collect? What is the model?\n",
        "\n",
        "2. DataBlock:\n",
        "*   blocks = (SourceBlockType, DestBlockType)\n",
        "*   get_items = get_image_files (how can we get the items)/\n",
        "*   splitter=RandomSplitter(valid_pct=0.2 //The test data size, seed=42 //ensures the same split for each epoch)\n",
        "*   get_y=parent_label (The independent variable is x and dependent variable is y). Basically tells the datablock how can we get the dependent variable's value.\n",
        "*   item_tfms=Resize(128) //Per item resize. Can also use batch_tfms to batch the transformations. This helps better utilize the GPU.\n",
        "\n",
        "3. DataLoaders: The actual object which contains the split. DataBlock is just a definition of sorts.\n",
        "\n",
        "4. Resizing directly may not be the best as we may miss crucial details. We can use RandomResizedCrop to resize the model.\n",
        "\n",
        "5. Data Augmentation: Data can be modified such that the image is slightly rotated/blurred etc. just to ensure the model learns effectively. Can be used when loading data onto the dataloaders. Although data is out of scope if the model was not trained on it.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ka3OLNjCb4z2"
      }
    }
  ]
}